{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ChatGPT Works Part 3: RLHF\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/RLHF-Implementation/blob/main/Notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "> Reinforcement Learning with Human Feedback, or RLHF, is a technique used to update a machine learning model based on human feedback\n",
    "\n",
    "The second and third step in the diagram below encompass RLHF:\n",
    "- The reward model is trained to predict the reward for each response using a supervised dataset of prompts and various responses in step 2\n",
    "- The reward model is used in the reinforcement learning setup in step 3 to predict the reward for each response on an unsupervised dataset of prompts\n",
    "\n",
    "![](./images/How%20chatGPT%20is%20trained.png)\n",
    "\n",
    "### Recap: What is Reinforcement Learning?\n",
    "\n",
    "> Reinforcement learning is where an agent (in our case, the AI system) interacts with an an environment (in our case, interacting with the chat interface by responding to prompts), and tries to maximise a reward which is receives for doing well (or a punishment for not doing well).\n",
    "\n",
    "![](./images/RL%20Formulation.png)\n",
    "\n",
    "\n",
    "In our case:\n",
    "- The action taken by the bot is the response it provides\n",
    "- The policy is the model that the chatbot uses to provide a response\n",
    "- The reward is generated by the reward model\n",
    "- The state is the chat so far\n",
    "\n",
    "![](./images/ChatGPT%20RL%20Formulation.png)\n",
    "\n",
    "More specifically:\n",
    "- Both the language model and the reward model are transformer neural networks\n",
    "- The reward model remains fixed, assuming that it's already encoded the values we want the model to align with\n",
    "- The language model is updated\n",
    "\n",
    "![](./images/RLHF%20NN%20Setup%20for%20LMs.png)\n",
    "\n",
    "> Note that in the InstructGPT paper, both the language model and the reward model were initialised using the parameters resulting from the SFT process performed earlier.\n",
    "\n",
    "### The Reward Model is Used to Encode Complex Behaviours that are Very Difficult to Define\n",
    "\n",
    "> It can be very difficult to define many of the behaviours that we want our AI systems to exhibit\n",
    "\n",
    "- What does it mean to be unbiased?\n",
    "- What does it mean to act professionally?\n",
    "- What does it mean to be ethical?\n",
    "\n",
    "> Instead of trying to explicity write out the rules for what each of these things, a better approach can be to learn them from human feedback\n",
    "\n",
    "It's hard to write the rules for these things, but it's relatively easy for a human to tell whether an output is biased, professional, or ethical.\n",
    "That's why the reward model is trained on human feedback (rankings of different responses to a given prompt). \n",
    "If the reward model is trained sufficiently to fit a dataset that prefers unbiased, ethical responses etc, then it should encode these complex behaviours.\n",
    "\n",
    "> The reward model is used to provide the reward used in the reinforcement learning setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "To implement the reinforcement learning loop, we'll need the dataset. Thanks to the reward model, which will provide the reward as a label for each response, we don't need human written labels for each of them. The dataset should simply return different prompts. The model will then complete them and the reward model will score them, before we use the reward to update the policy for generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class PromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prompts = pd.read_csv('prompt_dataset.csv')[\"Prompt\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]\n",
    "\n",
    "prompt_dataset = PromptDataset()\n",
    "prompt_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENABLE GPU RUNTIME NOW IF ON GOOGLE COLAB\n",
    "\n",
    "In the next few cells, we'll train and save the SFT language model and the reward model. The training can be massively accelerated by using a GPU (graphics processing unit - fast for parallel operations) instead of a CPU (central processing unit - fast for sequential operations).\n",
    "\n",
    "> Enable the GPU runtime on Google Colab now\n",
    "\n",
    "To do so, hit \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" -> \"GPU\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Pre-Trained Language Model\n",
    "\n",
    "By this point, we should already have performed supervised fine-tuning (SFT) on a large langauge model.\n",
    "\n",
    "Let's load in our fine-tuned language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import json\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "\n",
    "class SFTModel(GPT2LMHeadModel):\n",
    "    def __init__(self):\n",
    "        configuration = GPT2Config.from_pretrained(\n",
    "            'gpt2', output_hidden_states=False)\n",
    "        super().__init__(config=configuration)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "            \"gpt2\", config=configuration)  # Load the tokenizer\n",
    "        self.to(torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.to(self.device)  # Move the model to the GPU\n",
    "\n",
    "    def forward(self, prompt, response):\n",
    "        # Encode the data\n",
    "        entire_text = prompt + response\n",
    "        context_dict = self.tokenizer(\n",
    "            '<|startoftext|>' + entire_text + '<|endoftext|>',\n",
    "            #    truncation=True,\n",
    "            #    max_length=max_length,\n",
    "            #    padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor(context_dict.input_ids)\n",
    "        labels = torch.tensor(context_dict.input_ids)\n",
    "        attention_mask = torch.tensor(context_dict.attention_mask)\n",
    "\n",
    "        # Move to GPU\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "        # Run the model\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class SFTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Supervised Fine-Tuning Dataset\n",
    "\n",
    "    Returns:\n",
    "        prompt: str\n",
    "        response: str\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        with open(\"sft_dataset.json\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Defines the length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Defines how to get a sample from the dataset by indexing it.\n",
    "\n",
    "        Returns:\n",
    "            prompt: str\n",
    "            response: str\n",
    "        \"\"\"\n",
    "        return self.data[idx][\"prompt\"], self.data[idx][\"response\"]\n",
    "\n",
    "\n",
    "def train_and_save_SFT_model(epochs=10):\n",
    "\n",
    "    # Create the model\n",
    "    model = SFTModel()  # Load the model\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = SFTDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Create the optimizer\n",
    "    # as used in the InstructGPT paper\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=1e-5, betas=(0.9, 0.95))\n",
    "\n",
    "    # Set up logging\n",
    "    writer = SummaryWriter()  # for logging our loss to TensorBoard\n",
    "    # for setting the x-axis of our TensorBoard plots (loss vs. batch index)\n",
    "    batch_idx = 0\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Get the data\n",
    "            prompt, response = batch\n",
    "            prompt = prompt[0]\n",
    "            response = response[0]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(prompt, response)\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log the loss\n",
    "            # print(f\"Loss: {loss.item()}\", batch_idx)\n",
    "            writer.add_scalar(\"SFT Model Loss/train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "    torch.save(model.state_dict(), \"sft_model_params.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_SFT_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained and saved the SFT model, we need to load it in and set its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = SFTModel() # create model\n",
    "sft_state_dict = torch.load('sft_model_params.pt') # load model weights\n",
    "sft_model.load_state_dict(sft_state_dict) # set model weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Pre-Trained Reward Model\n",
    "\n",
    "By this point, we should have already trained a reward model that takes in a prompt and a response and produces a scalar reward - a measure of how good the response is for that context.\n",
    "\n",
    "Let's load in our reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def loss_function(preferred_response_reward, alternate_response_reward):\n",
    "    return -torch.mean(torch.log(torch.sigmoid(preferred_response_reward - alternate_response_reward)))\n",
    "\n",
    "\n",
    "def create_response_pairs():\n",
    "\n",
    "    data = pd.read_csv('reward_dataset.csv', sep=\"|\")\n",
    "\n",
    "    data = data.to_dict(orient=\"records\")\n",
    "    response_pairs = []\n",
    "\n",
    "    for row in data:\n",
    "        prompt = row[\"Prompt\"]\n",
    "        response_pairs.append(\n",
    "            (prompt, row[\"Most preferable response\"], row[\"Somewhat preferable response\"]))\n",
    "        response_pairs.append(\n",
    "            (prompt, row[\"Most preferable response\"], row[\"Least preferable response\"]))\n",
    "        response_pairs.append(\n",
    "            (prompt, row[\"Somewhat preferable response\"], row[\"Least preferable response\"]))\n",
    "\n",
    "    return response_pairs\n",
    "\n",
    "\n",
    "class RewardDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the dataset.\"\"\"\n",
    "        self.response_pairs = create_response_pairs()\n",
    "        print(\"Number of response pairs:\", len(self.response_pairs))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self.response_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the example in the dataset at the given index.\"\"\"\n",
    "\n",
    "        # Get the response pair at the given index\n",
    "        response_pair = self.response_pairs[idx]\n",
    "        prompt, preferred_response, alternate_response = response_pair\n",
    "\n",
    "        # Return the preferred response, alternate response\n",
    "        return prompt, preferred_response, alternate_response\n",
    "\n",
    "\n",
    "class RewardModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.backbone = GPT2Model.from_pretrained('gpt2')\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.regression_head = torch.nn.Linear(768, 1)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, context, response):\n",
    "        \"\"\"\n",
    "        Returns a scalar value representing the reward for this response, given the context.\n",
    "        Args:\n",
    "            context (str): The context. aka. the prompt.\n",
    "            response (str): The response. aka. the response to the prompt.\n",
    "        Returns:\n",
    "            float: The reward for generating this response given the context.    \n",
    "        \"\"\"\n",
    "\n",
    "        entire_text = context + response\n",
    "        context_dict = self.tokenizer(\n",
    "            '<|startoftext|>' + entire_text + '<|endoftext|>',\n",
    "            #    truncation=True,\n",
    "            #    max_length=max_length,\n",
    "            #    padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor(context_dict.input_ids)\n",
    "        attention_mask = torch.tensor(context_dict.attention_mask)\n",
    "\n",
    "        # Move to GPU\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "        # Forward pass\n",
    "        gpt2_outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        all_output_vectors = gpt2_outputs.last_hidden_state\n",
    "        last_output_vector = all_output_vectors[-1]\n",
    "\n",
    "        # add batch_size dimension\n",
    "        last_output_vector = last_output_vector.unsqueeze(0)\n",
    "        reward = self.regression_head(last_output_vector)\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "def train_and_save_reward_model(epochs=10):\n",
    "\n",
    "    model = RewardModel()\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = RewardDataset()\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=1e-5, betas=(0.9, 0.95))  # as used in the InstructGPT paper\n",
    "\n",
    "    # Set up logging\n",
    "    writer = SummaryWriter()  # for logging our loss to TensorBoard\n",
    "    # for setting the x-axis of our TensorBoard plots (loss vs. batch index)\n",
    "    batch_idx = 0\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for batch in tqdm(dataset):\n",
    "\n",
    "            prompt, preferred_response, alternate_response = batch\n",
    "\n",
    "            preferred_response_reward = model(prompt, preferred_response)\n",
    "            alternate_response_reward = model(prompt, alternate_response)\n",
    "\n",
    "            loss = loss_function(preferred_response_reward,\n",
    "                                 alternate_response_reward)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            writer.add_scalar(\"Reward Model Loss/Train\",\n",
    "                              loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "            # torch.save(model.state_dict(),\n",
    "            #            f\"epoch-{epoch}-reward_model_params.pt\")\n",
    "    torch.save(model.state_dict(), \"reward_model_params.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_save_reward_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained and saved the reward model, we need to load it in and set its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RewardModel()  # create model\n",
    "reward_state_dict = torch.load('reward_model.pt')  # load model weights\n",
    "reward_model.load_state_dict(reward_state_dict)  # set model weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Initial Attempt - Train the Policy to Maximise the Reward\n",
    "\n",
    "The overall objective that RLHF optimises is rather complicated, so before we optimise for that, let's try to simply maximise the reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_maximisation_objective(prompt, response, reward_model):\n",
    "    \"\"\"Returns the reward maximisation objective for the given prompt and response.\"\"\"\n",
    "\n",
    "    # Set the reward model to evaluation mode (Disables dropout and batch norm)\n",
    "    reward_model.eval()\n",
    "\n",
    "    # Get the reward for the response\n",
    "    reward = reward_model(prompt, response)\n",
    "\n",
    "    # this is a trivial function right now, \n",
    "    # but it highlights that the maximisation objective could be any function like this\n",
    "    # and could include more terms\n",
    "\n",
    "    # Return the reward\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_and_save_RLHF_model():\n",
    "    \"\"\"Trains the RLHF model and saves it to disk.\"\"\"\n",
    "\n",
    "    # Set up logging\n",
    "    writer = SummaryWriter()\n",
    "    batch_idx = 0\n",
    "\n",
    "    # Create the prompt dataset\n",
    "    prompt_dataset = PromptDataset()\n",
    "\n",
    "    # Create the reward model\n",
    "    reward_model = RewardModel()\n",
    "\n",
    "    # Create the SFT model\n",
    "    sft_model = SFTModel()\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(sft_model.parameters(), lr=1e-4) # make sure to only train the SFT model, not the reward model which should be frozen\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(10):\n",
    "        for prompt in prompt_dataset:\n",
    "            # Get the response\n",
    "            response = sft_model(prompt)\n",
    "\n",
    "            # Get the reward maximisation objective\n",
    "            objective = reward_maximisation_objective(prompt, response, reward_model)\n",
    "\n",
    "            # Torch minimises objectives, and we need to maximise the reward, so we negate the objective\n",
    "            objective = -objective\n",
    "\n",
    "            # Backpropagate the objective\n",
    "            objective.backward()\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log the objective\n",
    "            writer.add_scalar(\n",
    "                \"RLHF Model Objective/train\", \n",
    "                -objective.item(), # Remember to negate the objective to get the actual value\n",
    "                batch_idx\n",
    "            )\n",
    "            batch_idx += 1\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(sft_model.state_dict(), 'rlhf_model_params.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Term to the Loss Function to Minimise How Much the Model can Deviate from the Original SFT parameters \n",
    "\n",
    "The overall loss function used in the instructGPT paper contains more terms in the loss function.\n",
    "\n",
    "One of those additional terms is used to make sure that the final model tuned with RL stays close to the initial parameterisation produced by the fine-tuning.\n",
    "Without this, during the RL optimisation, the model can begin to predict gibberish that no longer well models the distribution of the language, but that tricks the reward model.\n",
    "\n",
    "ChatGPT uses the PPO reinforcement learning algorithm objective. \n",
    "This is the objective that it tries to maximise using gradient descent.\n",
    "\n",
    "> Maximising the objective is the same as minimising the negative objective.\n",
    "\n",
    "![](./images/RLHF%20LM%20PPO%20Objective.png)\n",
    "<!-- \n",
    "## The REINFORCE Obective\n",
    "\n",
    "> REINFORCE is a reinforcement learning algorithm that PPO (the algorithm we will use) builds upon\n",
    "\n",
    "The REINFORCE objective function is as follows: -->\n",
    "\n",
    "<!-- ## PPO -->\n",
    "\n",
    "<!-- - Averaged over a batch of different responses\n",
    "- Rewards ratio: $\\frac{reward \\ with\\ new\\ params}{reward\\ with\\ old\\ params}$ for the same input prompt    \n",
    "- Multiplied by the advantage function\n",
    "- Clipped to not change the policy too much - so that the new policy is in proximity of the other in terms of how much the reward will change -->\n",
    "<!-- \n",
    "### The Rewards Ratio\n",
    "\n",
    "![](./images/Rewards%20Ratio.png)\n",
    "\n",
    "- If the reward ratio is > 1, then it means that taking action $a$ in state $s$ is more likely with the new policy compared to the old one.\n",
    "- If the reward ratio is < 1, then it means that taking action $a$ in state $s$ is less likely with the new policy compared to the old one.\n",
    "\n",
    "> The ratio of the rewards tells you how drastically the policy is changing per update.\n",
    "\n",
    "### Clipping the Reward\n",
    "\n",
    "> If the policy changes too much, the  -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "- Update the reward model so that it uses the SFT model as a starting point, instead of using GPT2\n",
    "- Log the generated responses to tensorboard during training\n",
    "- Implement proxy batching in your SFT model and reward model to get more accurate gradient updates by performing several forward passes, allowing gradients to accumulate, before taking their mean and calling `optimiser.step()`\n",
    "- Get the logits from your model and use them to compute the second term in the loss function by using the chain rule of conditional probability. See how this affects the responses generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if the reward model is wrong?\n",
    "\n",
    "The policy is optimised to maximise the reward model score.\n",
    "\n",
    "That means everything depends on the reward model being accurate.\n",
    "\n",
    "Assuming that the reward model is accurate, with too much fine tuning via RLHF, the policy can begin to overfit to the reward model and in fact produce responses less preferred by humans.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
