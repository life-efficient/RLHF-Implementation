{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ChatGPT Works Part 3: RLHF\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/RLHF-Implementation/blob/main/Notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "> Reinforcement Learning with Human Feedback, or RLHF, is a technique used to update a machine learning model based on human feedback\n",
    "\n",
    "The second and third step in the diagram below encompass RLHF:\n",
    "- The reward model is trained to predict the reward for each response using a supervised dataset of prompts and various responses in step 2\n",
    "- The reward model is used in the reinforcement learning setup in step 3 to predict the reward for each response on an unsupervised dataset of prompts\n",
    "\n",
    "![](./images/How%20chatGPT%20is%20trained.png)\n",
    "\n",
    "### Recap: What is Reinforcement Learning?\n",
    "\n",
    "> Reinforcement learning is where an agent (in our case, the AI system) interacts with an an environment (in our case, interacting with the chat interface by responding to prompts), and tries to maximise a reward which is receives for doing well (or a punishment for not doing well).\n",
    "\n",
    "![](./images/RL%20Formulation.png)\n",
    "\n",
    "### The Reward Model is Used to Encode Complex Behaviours that are Very Difficult to Define\n",
    "\n",
    "> It can be very difficult to define many of the behaviours that we want our AI systems to exhibit\n",
    "\n",
    "- What does it mean to be unbiased?\n",
    "- What does it mean to act professionally?\n",
    "- What does it mean to be ethical?\n",
    "\n",
    "> Instead of trying to explicity write out the rules for what each of these things, a better approach can be to learn them from human feedback\n",
    "\n",
    "It's hard to write the rules for these things, but it's relatively easy for a human to tell whether an output is biased, professional, or ethical.\n",
    "That's why the reward model is trained on human feedback (rankings of different responses to a given prompt). \n",
    "If the reward model is trained sufficiently to fit a dataset that prefers unbiased, ethical responses etc, then it should encode these complex behaviours.\n",
    "\n",
    "> The reward model is used to provide the reward used in the reinforcement learning setup\n",
    "\n",
    "## The Overall Loss Function\n",
    "\n",
    "ChatGPT uses the PPO reinforcement learning algorithm objective. \n",
    "This is the thing that it tries to maximise.\n",
    "\n",
    "![](./images/PPOLoss.png)\n",
    "<!-- \n",
    "## The REINFORCE Obective\n",
    "\n",
    "> REINFORCE is a reinforcement learning algorithm that PPO (the algorithm we will use) builds upon\n",
    "\n",
    "The REINFORCE objective function is as follows: -->\n",
    "\n",
    "<!-- ## PPO -->\n",
    "\n",
    "- Averaged over a batch of different responses\n",
    "- Rewards ratio: $\\frac{reward \\ with\\ new\\ params}{reward\\ with\\ old\\ params}$ for the same input prompt    \n",
    "- Multiplied by the advantage function\n",
    "- Clipped to not change the policy too much - so that the new policy is in proximity of the other in terms of how much the reward will change\n",
    "\n",
    "### The Rewards Ratio\n",
    "\n",
    "![](./images/Rewards%20Ratio.png)\n",
    "\n",
    "- If the reward ratio is > 1, then it means that taking action $a$ in state $s$ is more likely with the new policy compared to the old one.\n",
    "- If the reward ratio is < 1, then it means that taking action $a$ in state $s$ is less likely with the new policy compared to the old one.\n",
    "\n",
    "> The ratio of the rewards tells you how drastically the policy is changing per update.\n",
    "\n",
    "### Clipping the Reward\n",
    "\n",
    "> If the policy changes too much, the \n",
    "\n",
    "\n",
    "## What if the reward model is wrong?\n",
    "\n",
    "The policy is optimised to maximise the reward model score.\n",
    "\n",
    "That means everything depends on the reward model being accurate.\n",
    "\n",
    "Assuming that the reward model is accurate, with too much fine tuning via RLHF, the policy can begin to overfit to the reward model and in fact produce responses less preferred by humans."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "To implement the reinforcement learning loop, we'll need the dataset. Thanks to the reward model, which will provide the reward as a label for each response, we don't need human written labels for each of them. The dataset should simply return different prompts. The model will then complete them and the reward model will score them, before we use the reward to update the policy for generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class PromptDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prompts = pd.read_csv('prompt_dataset.csv')[\"Prompt\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]\n",
    "\n",
    "prompt_dataset = PromptDataset()\n",
    "prompt_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Pre-Trained Language Model\n",
    "\n",
    "By this point, we should already have performed supervised fine-tuning (SFT) on a large langauge model.\n",
    "\n",
    "Let's load in our fine-tuned language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SFT_model import train_and_save_SFT_model, SFTModel\n",
    "\n",
    "train_and_save_SFT_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained and saved the SFT model, we need to load it in and set its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = SFTModel() # create model\n",
    "sft_state_dict = torch.load('sft_model.pt') # load model weights\n",
    "sft_model.load_state_dict(sft_state_dict) # set model weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Pre-Trained Reward Model\n",
    "\n",
    "By this point, we should have already trained a reward model that takes in a prompt and a response and produces a scalar reward - a measure of how good the response is for that context.\n",
    "\n",
    "Let's load in our reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_model import train_and_save_reward_model, RewardModel\n",
    "\n",
    "train_and_save_reward_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained and saved the reward model, we need to load it in and set its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RewardModel()  # create model\n",
    "reward_state_dict = torch.load('reward_model.pt')  # load model weights\n",
    "reward_model.load_state_dict(reward_state_dict)  # set model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
